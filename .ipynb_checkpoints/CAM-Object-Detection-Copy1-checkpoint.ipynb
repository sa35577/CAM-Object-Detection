{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM and Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: \n",
    "Sat Arora \\\n",
    "Richard Fan\n",
    "\n",
    "### Project Goal:\n",
    "\"CAM and object detection\". First, you should implement some standard method for CAM for some (simple) classification network trained on image-level tags. You should also obtain object detection (spacial localization of the object approximate \"center\"). You should apply your approach to one specific object type (e.g. faces, or anything else). Training should be done on image-level tags (e.g. face, no face). You can come up with your specialized dataset, but feel free to use subsets of standard data. You can also test the ideas on real datasets where label noise is presemt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model used: ResNet 18\n",
    "model = models.resnet(pretrained=True)\n",
    "final_convolution_layer = 'layer4'\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('river_hand.jpeg')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "    \n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Remove the fully connected layer\n",
    "model = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread('image_1.jpg')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_img = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Forward pass to get feature maps\n",
    "with torch.no_grad():\n",
    "    feature_maps = model(input_img)\n",
    "\n",
    "# Get the weights of the final convolutional layer\n",
    "final_conv_layer = None\n",
    "for layer in reversed(model):\n",
    "    if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "        final_conv_layer = layer\n",
    "        break\n",
    "\n",
    "if final_conv_layer is None:\n",
    "    raise ValueError(\"Final convolutional layer not found in the model.\")\n",
    "\n",
    "final_conv_layer_weights = final_conv_layer.weight.detach().cpu()\n",
    "\n",
    "# Compute the class activation map (CAM)\n",
    "cam = np.zeros((feature_maps.shape[2], feature_maps.shape[3]), dtype=np.float32)\n",
    "for i in range(final_conv_layer_weights.size(0)):\n",
    "    weight = final_conv_layer_weights[i].detach().cpu().numpy()\n",
    "    cam += np.sum(weight * feature_maps.squeeze(0)[i].cpu().numpy(), axis=0)\n",
    "\n",
    "cam = np.maximum(cam, 0)  # ReLU activation\n",
    "cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "\n",
    "# Apply heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "print(heatmap.shape)\n",
    "superimposed_img = heatmap * 0.4 + img.astype('float32') * 0.6\n",
    "superimposed_img = superimposed_img / superimposed_img.max()\n",
    "\n",
    "# Display the original image and the image with the heatmap\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('CAM', np.uint8(255 * superimposed_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
