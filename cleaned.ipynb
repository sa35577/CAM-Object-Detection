{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM and Object Detection (modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: \n",
    "Sat Arora, sat.arora@uwaterloo.ca \\\n",
    "Richard Fan, r43fan@uwaterloo.ca\n",
    "\n",
    "### Project Goal:\n",
    "\"CAM and object detection\". First, you should implement some standard method for CAM for some (simple) classification network trained on image-level tags. You should also obtain object detection (spacial localization of the object approximate \"center\"). You should apply your approach to one specific object type (e.g. faces, or anything else). Training should be done on image-level tags (e.g. face, no face). You can come up with your specialized dataset, but feel free to use subsets of standard data. You can also test the ideas on real datasets where label noise is present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Class Activation Maps (CAMs) is a very important tool and concept in Computer Vision. During classification, the goal of CAMs is to indicate the regions of the image that were used by a Convolutional Neural Network to lead it to classifying an image as containing a certain object.\n",
    "\n",
    "In order to understand what the Class Activation Maps do, this report will describe in detail the motivation, ideas & concepts that guide our process to making our own CAMs. Following this, we will do some deeper analysis of what happens in certain scenarios to better understand our algorithms output.\n",
    "\n",
    "The approach and motivation are inspired by [Learning Deep Features for Discriminative Localization](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) (Zhou, Khosla, Laperdriza, Oliva, Tarralba), a paper that was released in 2016. The appraoch is extended by testing our own NNs along with common ones (ResNet18) and analyzing impacts of more special scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "\n",
    "Sat Arora: sat.arora@uwaterloo.ca\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "\n",
    "Richard Fan: r43fan@uwaterloo.ca\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "\n",
    "Fun fact: We are born on the same day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "As mentioned in the Abstract, the goal of CAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a uniform size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load train and test datasets\n",
    "train_dataset = ImageFolder('Dataset/train', transform=transform)\n",
    "test_dataset = ImageFolder('Dataset/test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, test_loader, num_epochs=10):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = train_loss / len(train_dataset)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_correct / train_total * 100:.2f}%')\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_loss_total += loss.item() * images.size(0)\n",
    "\n",
    "\n",
    "    accuracy = test_correct / test_total\n",
    "    test_loss = test_loss_total / len(test_dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the image\n",
    "# image_file = 'sat.png'\n",
    "# image_file = '2 faces.png'\n",
    "# image_file = 'river_hand.jpeg'\n",
    "# image_file = 'image_2.jpg'\n",
    "# image_file = 'tejas.jpg'\n",
    "# image_file = 'shahan.jpg'\n",
    "# image_file = 'osama.jpg'\n",
    "# image_file = 'Human1250 copy.png'\n",
    "\n",
    "def test(model, original_img):\n",
    "    model.eval()\n",
    "    # original_img = cv2.imread(image_file)\n",
    "\n",
    "    if original_img is not None:\n",
    "        print(\"Image loaded successfully!\")\n",
    "    else:\n",
    "        print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "    features_blobs = []\n",
    "    def hook_feature(module, input, output):\n",
    "        features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "    model._modules.get('layer4').register_forward_hook(hook_feature)\n",
    "\n",
    "    img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_img = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    # Forward pass to get feature maps\n",
    "    with torch.no_grad():\n",
    "        feature_maps = model(input_img)\n",
    "\n",
    "    params = list(model.parameters())\n",
    "    weight = np.squeeze(params[-2].data.cpu().numpy())\n",
    "    cam = weight[0].dot(features_blobs[0].reshape(-1, 7 * 7))\n",
    "\n",
    "    print(\"cam\", cam)\n",
    "    cam = cam.reshape(7, 7)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam = cam / np.max(cam)\n",
    "    cam = np.uint8(255 * cam)\n",
    "    # cam = cv2.resize(cam, (256, 256))\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0])) \n",
    "    print(\"shape\", cam.shape)\n",
    "\n",
    "    # Apply heatmap on the original image\n",
    "    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.1506, Train Accuracy: 94.90%\n",
      "Epoch [2/10], Train Loss: 0.0468, Train Accuracy: 96.79%\n",
      "Epoch [3/10], Train Loss: 0.0455, Train Accuracy: 97.40%\n",
      "Epoch [4/10], Train Loss: 0.0171, Train Accuracy: 97.92%\n",
      "Epoch [5/10], Train Loss: 0.0046, Train Accuracy: 98.32%\n",
      "Epoch [6/10], Train Loss: 0.0848, Train Accuracy: 98.08%\n",
      "Epoch [7/10], Train Loss: 0.0213, Train Accuracy: 98.23%\n",
      "Epoch [8/10], Train Loss: 0.0067, Train Accuracy: 98.42%\n",
      "Epoch [9/10], Train Loss: 0.0249, Train Accuracy: 98.52%\n",
      "Epoch [10/10], Train Loss: 0.0024, Train Accuracy: 98.66%\n",
      "Test Loss: 0.0082, Test Accuracy: 99.43%\n",
      "Image loaded successfully!\n",
      "cam [-1.6349951  -2.3052027  -1.4116313   0.29589647  0.8998902   0.8346766\n",
      "  0.8372079  -2.8732111  -3.6218104  -1.8234608  10.063262   16.008093\n",
      " 13.748569    4.724561   -2.5949268  -3.1738238   1.5728251  24.08196\n",
      " 36.391167   31.585346   10.71907    -2.4874299  -3.0573514   1.9726497\n",
      " 25.105227   36.909855   31.18098     9.732546   -2.3602977  -2.9359398\n",
      " -0.68466973 12.595033   17.917036   15.276004    4.501574   -2.2008057\n",
      " -3.148291   -2.3185232  -0.527544    0.2486024  -0.42219973 -0.72126997\n",
      " -1.3378776  -2.1440184  -1.7352381  -1.1881952  -1.0514234  -1.414033\n",
      " -1.1051615 ]\n",
      "shape (4032, 3024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 output classes: face and no-face\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training model\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trained_model = train(model, optimizer, criterion, train_loader, test_loader, num_epochs=10)\n",
    "\n",
    "# Generate heatmap\n",
    "image_file = 'sat.png'\n",
    "original_img = cv2.imread(image_file)\n",
    "heatmap = test(trained_model, original_img)\n",
    "result = heatmap * 0.3 + original_img * 0.5\n",
    "cv2.imwrite('CAM3.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.4599, Train Accuracy: 82.20%\n",
      "Epoch [2/20], Train Loss: 0.2779, Train Accuracy: 85.55%\n",
      "Epoch [3/20], Train Loss: 0.2487, Train Accuracy: 87.03%\n",
      "Epoch [4/20], Train Loss: 0.2026, Train Accuracy: 88.28%\n",
      "Epoch [5/20], Train Loss: 0.1556, Train Accuracy: 89.33%\n",
      "Epoch [6/20], Train Loss: 0.1403, Train Accuracy: 90.17%\n",
      "Epoch [7/20], Train Loss: 0.1361, Train Accuracy: 90.87%\n",
      "Epoch [8/20], Train Loss: 0.1258, Train Accuracy: 91.40%\n",
      "Epoch [9/20], Train Loss: 0.1013, Train Accuracy: 91.96%\n",
      "Epoch [10/20], Train Loss: 0.0957, Train Accuracy: 92.46%\n",
      "Epoch [11/20], Train Loss: 0.0937, Train Accuracy: 92.81%\n",
      "Epoch [12/20], Train Loss: 0.0731, Train Accuracy: 93.18%\n",
      "Epoch [13/20], Train Loss: 0.0493, Train Accuracy: 93.56%\n",
      "Epoch [14/20], Train Loss: 0.0520, Train Accuracy: 93.89%\n",
      "Epoch [15/20], Train Loss: 0.0879, Train Accuracy: 94.09%\n",
      "Epoch [16/20], Train Loss: 0.0523, Train Accuracy: 94.32%\n",
      "Epoch [17/20], Train Loss: 0.0473, Train Accuracy: 94.55%\n",
      "Epoch [18/20], Train Loss: 0.0458, Train Accuracy: 94.76%\n",
      "Epoch [19/20], Train Loss: 0.0325, Train Accuracy: 94.97%\n",
      "Epoch [20/20], Train Loss: 0.0245, Train Accuracy: 95.19%\n",
      "Test Loss: 0.0794, Test Accuracy: 97.17%\n",
      "Image loaded successfully!\n",
      "cam [-2.5709114  -1.4926351  -1.4611725   7.2400203  15.959067   -1.122746\n",
      " -1.543579   -4.107515   -0.4643517  -0.5524773   2.6231036   5.1530786\n",
      "  2.2421646   2.9945703  -2.473732   -0.46083125 -0.18745978  3.9427822\n",
      " 21.028812   20.204258    6.8165693  -2.5753207   2.35147     0.9624369\n",
      " 10.188356   24.422752   28.27668     4.9132953  -4.117951   -1.4530157\n",
      " -1.9915115   2.895319   29.103718   20.525055   -0.2853802  -5.135816\n",
      " -0.8890018   0.10500002  0.79540324  4.293258   -1.7745693  -4.337475\n",
      "  7.2715774   4.8602843   8.193576    3.0957193  -1.6181742  -3.5905852\n",
      " -4.891728  ]\n",
      "shape (4032, 3024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a ResNet18 model\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 output classes: face and no-face\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training model\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trained_model = train(model, optimizer, criterion, train_loader, test_loader, num_epochs=20)\n",
    "\n",
    "# Generate heatmap\n",
    "image_file = 'sat.png'\n",
    "original_img = cv2.imread(image_file)\n",
    "heatmap = test(trained_model, original_img)\n",
    "result = heatmap * 0.3 + original_img * 0.5\n",
    "cv2.imwrite('CAM3.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
