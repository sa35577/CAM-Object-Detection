{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM and Object Detection (modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: \n",
    "Sat Arora, sat.arora@uwaterloo.ca \\\n",
    "Richard Fan, r43fan@uwaterloo.ca\n",
    "\n",
    "### Project Goal:\n",
    "\"CAM and object detection\". First, you should implement some standard method for CAM for some (simple) classification network trained on image-level tags. You should also obtain object detection (spacial localization of the object approximate \"center\"). You should apply your approach to one specific object type (e.g. faces, or anything else). Training should be done on image-level tags (e.g. face, no face). You can come up with your specialized dataset, but feel free to use subsets of standard data. You can also test the ideas on real datasets where label noise is present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Class Activation Maps (CAMs) is a very important tool and concept in Computer Vision. During classification, the goal of CAMs is to indicate the regions of the image that were used by a Convolutional Neural Network to lead it to classifying an image as containing a certain object.\n",
    "\n",
    "In order to understand what the Class Activation Maps do, this report will describe in detail the motivation, ideas & concepts that guide our process to making our own CAMs. Following this, we will do some deeper analysis of what happens in certain scenarios to better understand our algorithms output.\n",
    "\n",
    "The approach and motivation are inspired by [Learning Deep Features for Discriminative Localization](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) (Zhou, Khosla, Laperdriza, Oliva, Tarralba), a paper that was released in 2016. The appraoch is extended by testing our own NNs along with common ones (ResNet18) and analyzing impacts of more special scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "\n",
    "Sat Arora: sat.arora@uwaterloo.ca\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "\n",
    "Richard Fan: r43fan@uwaterloo.ca\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "- INSERT HERE\n",
    "\n",
    "Fun fact: We are born on the same day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "As mentioned in the Abstract, the goal of CAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a uniform size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load train and test datasets\n",
    "train_dataset = ImageFolder('Dataset/train', transform=transform)\n",
    "test_dataset = ImageFolder('Dataset/test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satarora/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/satarora/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 output classes: face and no-face\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fanrongqi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 0.0270, Train Accuracy: 98.96%\n",
      "Test Loss: 0.0189, Test Accuracy: 99.43%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "num_epochs = 1\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = train_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_correct / train_total * 100:.2f}%')\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_loss_total += loss.item() * images.size(0)\n",
    "\n",
    "\n",
    "accuracy = test_correct / test_total\n",
    "test_loss = test_loss_total / len(test_dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.72%\n",
      "output tensor([[ 15.4635, -13.6419]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "1.000 -> face\n",
      "0.000 -> no_face\n",
      "output CAM.jpg for the top1 prediction: face\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "finalconv_name = 'layer4'\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "LABELS_file = 'imagenet-simple-labels.json'\n",
    "image_file = 'sat.png'\n",
    "\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(model.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file)\n",
    "if img_pil.mode == \"RGBA\":\n",
    "    img_pil = img_pil.convert(\"RGB\")\n",
    "img_tensor = preprocess(img_pil)\n",
    "# print(\"img\", img_tensor)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0)).to(device)\n",
    "logit = model(img_variable)\n",
    "\n",
    "\n",
    "print(\"output\", logit)\n",
    "\n",
    "classes = ['face', 'no_face']\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.cpu().numpy()\n",
    "idx = idx.cpu().numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 2):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM2.jpg', result)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully!\n",
      "cam [-8.54738116e-01 -2.30321601e-01  4.08209354e-01  2.06184077e+00\n",
      "  4.39873171e+00  4.31108570e+00  1.99328351e+00 -5.00323355e-01\n",
      "  2.61244386e-01  1.00852227e+00  2.16180782e+01  5.01737480e+01\n",
      "  5.47186584e+01  2.83886700e+01 -2.35522315e-01  7.71400154e-01\n",
      "  1.62730956e+00  3.93912125e+01  8.71552505e+01  9.43690720e+01\n",
      "  4.78319778e+01 -2.80187964e-01  5.12618780e-01  1.65669107e+00\n",
      "  3.25470695e+01  7.29792786e+01  7.58039474e+01  3.63722076e+01\n",
      "  7.82242343e-02  7.14588284e-01  1.57501221e+00  1.26094179e+01\n",
      "  2.85214157e+01  2.77791500e+01  1.21975765e+01  4.63668734e-01\n",
      "  9.52644825e-01  1.84224355e+00  2.39323425e+00  2.61379933e+00\n",
      "  1.45871854e+00 -5.59597351e-02  1.21139541e-01  1.15085649e+00\n",
      "  2.25656962e+00  2.31743789e+00  6.57429576e-01 -8.92111957e-01\n",
      " -1.16333926e+00]\n",
      "shape (4032, 3024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess the image\n",
    "original_img = cv2.imread('sat.png')\n",
    "# img = cv2.imread('2 faces.png')\n",
    "# img = cv2.imread('river_hand.jpeg')\n",
    "# img = cv2.imread('image_2.jpg')\n",
    "# img = cv2.imread('tejas.jpg')\n",
    "# img = cv2.imread('shahan.jpg')\n",
    "# img = cv2.imread('osama.jpg')\n",
    "# img = cv2.imread('Human1250 copy.png')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "model._modules.get('layer4').register_forward_hook(hook_feature)\n",
    "\n",
    "img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_img = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# Forward pass to get feature maps\n",
    "with torch.no_grad():\n",
    "    feature_maps = model(input_img)\n",
    "\n",
    "params = list(model.parameters())\n",
    "weight = np.squeeze(params[-2].data.cpu().numpy())\n",
    "cam = weight[0].dot(features_blobs[0].reshape(-1, 7 * 7))\n",
    "\n",
    "print(\"cam\", cam)\n",
    "cam = cam.reshape(7, 7)\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "cam = np.uint8(255 * cam)\n",
    "# cam = cv2.resize(cam, (256, 256))\n",
    "cam = cv2.resize(cam, (img.shape[1], img.shape[0])) \n",
    "print(\"shape\", cam.shape)\n",
    "\n",
    "# Apply heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + original_img * 0.5\n",
    "cv2.imwrite('CAM3.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
