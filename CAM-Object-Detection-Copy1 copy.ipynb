{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM and Object Detection (modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: \n",
    "Sat Arora, sat.arora@uwaterloo.ca \\\n",
    "Richard Fan, r43fan@uwaterloo.ca\n",
    "\n",
    "### Project Goal ***REMOVE THIS***:\n",
    "\"CAM and object detection\". First, you should implement some standard method for CAM for some (simple) classification network trained on image-level tags. You should also obtain object detection (spacial localization of the object approximate \"center\"). You should apply your approach to one specific object type (e.g. faces, or anything else). Training should be done on image-level tags (e.g. face, no face). You can come up with your specialized dataset, but feel free to use subsets of standard data. You can also test the ideas on real datasets where label noise is present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Class Activation Maps (CAMs) is a very important tool and concept in Computer Vision. During classification, the goal of CAMs is to indicate the regions of the image that were used by a Convolutional Neural Network to lead it to classifying an image as containing a certain object.\n",
    "\n",
    "In order to understand what the Class Activation Maps do, this report will describe in detail the motivation, ideas & concepts that guide our process to making our own CAMs. Following this, we will do some deeper analysis of what happens in certain scenarios to better understand the algorithm's output.\n",
    "\n",
    "The approach and motivation are inspired by [Learning Deep Features for Discriminative Localization](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) (Zhou, Khosla, Laperdriza, Oliva, Tarralba), a paper that was released in 2016. The appraoch is extended by comparing common classification CNNs (specifically, ResNet18) with a CNN that we train, analyzing the difference in image labelling and the heat map. These networks will be trained on a face/no-face dataset with labelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "\n",
    "Sat Arora: sat.arora@uwaterloo.ca\n",
    "- Initial ResNet18 model for object detection.\n",
    "- Heat map logic.\n",
    "- Experimenting with multiple objects of same type.\n",
    "\n",
    "Richard Fan: r43fan@uwaterloo.ca\n",
    "- Creating custom model (and fine-tuning) for object detection.\n",
    "- Heat map logic.\n",
    "- Testing difference between custom model and ResNet18 model.\n",
    "\n",
    "Fun fact: We are born on the same day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "### Conceptual Idea\n",
    "\n",
    "As mentioned in the Abstract, the goal of CAMs is to indicate the regions of images that is used by the CNN to identify a certain category.\n",
    "\n",
    "In the case of categorization, the last layer before output is a softmax layer (in order to determine which class is the most likely). Before running this last layer, if we run a technique called **Global Average Pooling (GAP)** on the convolutional feature maps at this point, then we can use these as features for a fully-connected layer that produces our categorization.\n",
    "\n",
    "Note: The idea of GAP is straight forward. An implementation can be seen here:\n",
    "$$\\text{GAP}(F_d) = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} F_d(i, j)$$\n",
    "Simply put, it averages the values of the maps into a singular number, and by doing so it reduces the dimensionality of the image.\n",
    "\n",
    "With this structure, we can leverage our knowledge of how the softmax works: we can project the weights of the output layer onto the convolution feature maps. This essentially leaves us with a heatmap of the \"most important\" features (since higher weights in the classification will be where the object is). This technique is known as \"Class Activation Mapping\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can this be more formally seen?\n",
    "Say that $\\forall (x,y)$, the activation of unit $k$ in the last convolutional layer in the CNN is $f_k(x,y)$. Then, after performing GAP, we have the average for unit $k$ to be $$F^k = \\sum_{x,y}{f_k(x,y)}$$\n",
    "\n",
    "Thus, we have that for some arbitrary class $c$, the input to the softmax in the final decision layer is $$S_c = \\sum_k{w_k^cF_k}$$ where $w_k^c$ is exactly the \"importance\", or weight, of class $c$ for the unit $k$. Recall that the otuput of softmax is thus $$\\frac{\\exp(S_c)}{\\sum_{c_0}{\\exp(S_{c_0})}}$$ for class $c$. If we plug in $F^k = \\sum_{x,y}{f_k(x,y)}$, we get $$S_c = \\sum_{x,y}{\\sum_k{w_k^cf_k(x,y)}}$$\n",
    "\n",
    "Define $M_c$ to be the CAM for $c$, with each spatial element $M_c(x,y) = \\sum_k{w_k^cf_k(x,y)}$. Then we can rewrite the definition of the class score $S_c$ to be $$S_c = \\sum_{x,y}{M_c(x,y)}$$\n",
    "\n",
    "As such, we see that $M_c(x,y)$ is exactly the importance of the activation for $c$ at spatial coordinate $(x,y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this mean?\n",
    "\n",
    "Thus, we can conclude that $f_k$ will be the map of the persence of the visual pattern corresponding to the location of the object. We have that the CAM is a weighted linear sum of these visual patterns, and so by upsamimagepling the CAM to the size of the input image, we can identify the image regions that played the biggest influence in the particular category. \n",
    "\n",
    "*Or, by a simple rethought, the regions that are highlighted correspond to the class that the CNN describes this image to be.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Libraries\n",
    "\n",
    "Many libraries used in our implementation would be considered as \"standard\" in Computer Vision projects or courses, but we list out everything in the import order to get a better understanding of what each import is used for:\n",
    "\n",
    "- ``PIL``: Used to read images from a directory. This image will get passed into the tensor layers. \n",
    "\n",
    "- ``torch`` / ``torchvision``: The main libraries for PyTorch (along with its own packages). These provide pre-set models (like ResNet18), and ability to create transformations and our own CNNs. This is extensively used for manipulating our tensors (along with ``numpy``, which is more forward-facing as will be seen), providing loaders for our training and testing process, and to perform training & computations on CUDA/MPS (GPU configurations) or the CPU.\n",
    "\n",
    "- ``numpy``: Used to manipulate tensors from ``torch``, and acts as a middle layer to write data in a form that libarries such as ``cv2`` and ``PIL`` can understand. \n",
    "\n",
    "- ``cv2``: Used for dealing with image resizing, writing/drawing, and modifying. It is particularly useful in overlaying our heatmap on top of the image, and optionally writing an image to a directory for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use a dataset of face and non-face images found on Kaggle from Sagar Karar. To get this dataset and format it in the way that the program needs to read it, run the following commands.\n",
    "\n",
    "**Note**: The first step assumes that you have the ``kaggle`` package installed on pip. Otherwise, click on [this link to the dataset page](https://www.kaggle.com/datasets/sagarkarar/nonface-and-face-dataset) and download the dataset. This will replace the first line in the bash script below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kaggle datasets download -d sagarkarar/nonface-and-face-dataset\n",
    "unzip nonface-and-face-dataset.zip\n",
    "cd Dataset\n",
    "\n",
    "# Make directories to split face and no face labels (trained with human, cat, dog faces)\n",
    "mkdir face\n",
    "mkdir no_face\n",
    "mv Human* Cat* Dog* face/\n",
    "mv *.* no_face/\n",
    "\n",
    "\n",
    "# Creating directories\n",
    "mkdir -p train/face\n",
    "mkdir -p train/no_face\n",
    "mkdir -p test/face\n",
    "mkdir -p test/no_face\n",
    "\n",
    "\n",
    "# Move 80% of face images to train/face and 20% to test/face\n",
    "find face -type f | sort -R | head -n $(($(find face -type f | wc -l) * 80 / 100)) | xargs -I {} mv {} train/face/\n",
    "mv face/* test/face/\n",
    "\n",
    "# Move 80% of no face images to train/no_face and 20% to test/no_face\n",
    "find no_face -type f | sort -R | head -n $(($(find no_face -type f | wc -l) * 80 / 100)) | xargs -I {} mv {} train/no_face/\n",
    "mv no_face/* test/no_face/\n",
    "\n",
    "rm -r face no_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the data and manipulating its contents to restructure it, we have that the train and test datasets are in the ``Dataset/train`` and ``Dataset/folder`` respectively. Note that the data is transformed by reshaping it, converting it to a Tensor, and then normalizing it with the standardized `mean` and `std` from ImageNet, which is what the models are trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -1.2103, -1.1932, -1.2103],\n",
      "         [-2.1008, -2.1008, -2.1179,  ..., -1.2788, -1.2788, -1.2617],\n",
      "         [-2.1008, -2.1008, -2.0665,  ..., -1.2959, -1.3302, -1.2788],\n",
      "         ...,\n",
      "         [-1.8953, -1.9295, -1.8953,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-1.9124, -1.9295, -1.8953,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-1.9467, -1.9638, -1.9295,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -1.2829, -1.2654, -1.2829],\n",
      "         [-2.0182, -2.0182, -2.0357,  ..., -1.3529, -1.3529, -1.3354],\n",
      "         [-2.0182, -2.0182, -1.9832,  ..., -1.3704, -1.4055, -1.3529],\n",
      "         ...,\n",
      "         [-1.7906, -1.8256, -1.7906,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-1.8081, -1.8256, -1.7906,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-1.8431, -1.8606, -1.8256,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.0724, -1.0550, -1.0724],\n",
      "         [-1.7870, -1.7870, -1.8044,  ..., -1.1421, -1.1421, -1.1247],\n",
      "         [-1.7870, -1.7870, -1.7696,  ..., -1.1596, -1.1944, -1.1421],\n",
      "         ...,\n",
      "         [-1.6650, -1.6999, -1.6650,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.6824, -1.6999, -1.6650,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.7173, -1.7347, -1.6999,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a uniform size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load train and test datasets\n",
    "train_dataset = ImageFolder('Dataset/train', transform=transform)\n",
    "test_dataset = ImageFolder('Dataset/test', transform=transform)\n",
    "\n",
    "print(train_dataset[0])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satarora/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/satarora/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 output classes: face and no-face\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Train Loss: 0.0939, Train Accuracy: 96.84%\n",
      "Epoch [2/2], Train Loss: 0.0380, Train Accuracy: 97.73%\n",
      "Test Loss: 0.1219, Test Accuracy: 95.28%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "num_epochs = 2\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = train_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_correct / train_total * 100:.2f}%')\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_loss_total += loss.item() * images.size(0)\n",
    "\n",
    "\n",
    "accuracy = test_correct / test_total\n",
    "test_loss = test_loss_total / len(test_dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully!\n",
      "cam [-1.0258129  -1.3987672  -1.5977178  -1.7860192  -1.8843458  -2.0514984\n",
      " -1.7054107  -0.97524136 -1.0213745  -0.26057523  0.34875482 -0.3549348\n",
      " -1.6226544  -1.6017452  -0.72951573  0.55181843  7.2906237  13.969972\n",
      " 12.593851    4.783316   -0.7883249  -0.8047608   0.8720914  12.793606\n",
      " 24.015093   21.92626     9.450156   -0.30643874 -1.0357145  -0.20476629\n",
      "  8.717547   18.442791   17.652874    7.5965056  -0.9641609  -1.699307\n",
      " -1.9514683  -0.32045045  3.5392182   3.5824316  -0.884097   -1.9502213\n",
      " -1.5851424  -1.9714911  -2.0003047  -1.9613106  -1.9999803  -2.2184565\n",
      " -1.8343383 ]\n",
      "shape (2100, 1576)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# original_img = cv2.imread('sat.png')\n",
    "# img = cv2.imread('2 faces.png')\n",
    "# img = cv2.imread('river_hand.jpeg')\n",
    "# img = cv2.imread('image_2.jpg')\n",
    "original_img = cv2.imread('tejas.jpg')\n",
    "# img = cv2.imread('shahan.jpg')\n",
    "# img = cv2.imread('osama.jpg')\n",
    "# img = cv2.imread('Human1250 copy.png')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "model._modules.get('layer4').register_forward_hook(hook_feature)\n",
    "\n",
    "img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_img = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# Forward pass to get feature maps\n",
    "with torch.no_grad():\n",
    "    feature_maps = model(input_img)\n",
    "\n",
    "params = list(model.parameters())\n",
    "weight = np.squeeze(params[-2].data.cpu().numpy())\n",
    "cam = weight[0].dot(features_blobs[0].reshape(-1, 7 * 7))\n",
    "\n",
    "print(\"cam\", cam)\n",
    "cam = cam.reshape(7, 7)\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "cam = np.uint8(255 * cam)\n",
    "# cam = cv2.resize(cam, (256, 256))\n",
    "cam = cv2.resize(cam, (img.shape[1], img.shape[0])) \n",
    "print(\"shape\", cam.shape)\n",
    "\n",
    "# Apply heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + original_img * 0.5\n",
    "cv2.imwrite('CAM3.jpg', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this pre-trained ResNet-18 model runs reasonably well on some images. However, how would these same images compare if we disabled the pre-training on the model? In other words, if we don't load in pre-trained weights from training with ImageNet, how different would the same results be? We explore this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ResNet-18 model with pre-trained weights outperforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassify(dataset):\n",
    "    face_indices = [i for i in range(len(dataset)) if dataset[i][1] == 1]  # Assuming class 1 is for faces\n",
    "    non_face_indices = [i for i in range(len(dataset)) if dataset[i][1] == 0]  # Assuming class 0 is for non-faces\n",
    "    \n",
    "    # Misclassify 30 face images as non-faces\n",
    "    for _ in range(30):\n",
    "        idx = random.choice(face_indices)\n",
    "        dataset.targets[idx] = 0  # Change the label to non-face\n",
    "\n",
    "    # Misclassify 30 non-face images as faces\n",
    "    for _ in range(30):\n",
    "        idx = random.choice(non_face_indices)\n",
    "        dataset.targets[idx] = 1  # Change the label to face"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
