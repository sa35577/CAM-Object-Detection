{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM and Object Detection (modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: \n",
    "Sat Arora, sat.arora@uwaterloo.ca \\\n",
    "Richard Fan, r43fan@uwaterloo.ca\n",
    "\n",
    "### Project Goal ***REMOVE THIS***:\n",
    "\"CAM and object detection\". First, you should implement some standard method for CAM for some (simple) classification network trained on image-level tags. You should also obtain object detection (spacial localization of the object approximate \"center\"). You should apply your approach to one specific object type (e.g. faces, or anything else). Training should be done on image-level tags (e.g. face, no face). You can come up with your specialized dataset, but feel free to use subsets of standard data. You can also test the ideas on real datasets where label noise is present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Class Activation Maps (CAMs) is a very important tool and concept in Computer Vision. During classification, the goal of CAMs is to indicate the regions of the image that were used by a Convolutional Neural Network to lead it to classifying an image as containing a certain object.\n",
    "\n",
    "In order to understand what the Class Activation Maps do, this report will describe in detail the motivation, ideas & concepts that guide our process to making our own CAMs. Following this, we will do some deeper analysis of what happens in certain scenarios to better understand the algorithm's output.\n",
    "\n",
    "The approach and motivation are inspired by [Learning Deep Features for Discriminative Localization](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) (Zhou, Khosla, Laperdriza, Oliva, Tarralba), a paper that was released in 2016. The appraoch is extended by comparing common classification CNNs (specifically, ResNet18) with a CNN that we train, analyzing the difference in image labelling and the heat map. These networks will be trained on a face/no-face dataset with labelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "\n",
    "Sat Arora: sat.arora@uwaterloo.ca\n",
    "- Initial ResNet18 model for object detection.\n",
    "- Heat map logic.\n",
    "- Experimenting with multiple objects of same type.\n",
    "\n",
    "Richard Fan: r43fan@uwaterloo.ca\n",
    "- Creating custom model (and fine-tuning) for object detection.\n",
    "- Heat map logic.\n",
    "- Testing difference between custom model and ResNet18 model.\n",
    "\n",
    "Fun fact: We are born on the same day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "### Conceptual Idea\n",
    "\n",
    "As mentioned in the Abstract, the goal of CAMs is to indicate the regions of images that is used by the CNN to identify a certain category.\n",
    "\n",
    "In the case of categorization, the last layer before output is a softmax layer (in order to determine which class is the most likely). Before running this last layer, if we run a technique called **Global Average Pooling (GAP)** on the convolutional feature maps at this point, then we can use these as features for a fully-connected layer that produces our categorization.\n",
    "\n",
    "Note: The idea of GAP is straight forward. An implementation can be seen here:\n",
    "$$\\text{GAP}(F_d) = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} F_d(i, j)$$\n",
    "Simply put, it averages the values of the maps into a singular number, and by doing so it reduces the dimensionality of the image.\n",
    "\n",
    "With this structure, we can leverage our knowledge of how the softmax works: we can project the weights of the output layer onto the convolution feature maps. This essentially leaves us with a heatmap of the \"most important\" features (since higher weights in the classification will be where the object is). This technique is known as \"Class Activation Mapping\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can this be more formally seen?\n",
    "Say that $\\forall (x,y)$, the activation of unit $k$ in the last convolutional layer in the CNN is $f_k(x,y)$. Then, after performing GAP, we have the average for unit $k$ to be $$F^k = \\sum_{x,y}{f_k(x,y)}$$\n",
    "\n",
    "Thus, we have that for some arbitrary class $c$, the input to the softmax in the final decision layer is $$S_c = \\sum_k{w_k^cF_k}$$ where $w_k^c$ is exactly the \"importance\", or weight, of class $c$ for the unit $k$. Recall that the otuput of softmax is thus $$\\frac{\\exp(S_c)}{\\sum_{c_0}{\\exp(S_{c_0})}}$$ for class $c$. If we plug in $F^k = \\sum_{x,y}{f_k(x,y)}$, we get $$S_c = \\sum_{x,y}{\\sum_k{w_k^cf_k(x,y)}}$$\n",
    "\n",
    "Define $M_c$ to be the CAM for $c$, with each spatial element $M_c(x,y) = \\sum_k{w_k^cf_k(x,y)}$. Then we can rewrite the definition of the class score $S_c$ to be $$S_c = \\sum_{x,y}{M_c(x,y)}$$\n",
    "\n",
    "As such, we see that $M_c(x,y)$ is exactly the importance of the activation for $c$ at spatial coordinate $(x,y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this mean?\n",
    "\n",
    "Thus, we can conclude that $f_k$ will be the map of the persence of the visual pattern corresponding to the location of the object. We have that the CAM is a weighted linear sum of these visual patterns, and so by upsamimagepling the CAM to the size of the input image, we can identify the image regions that played the biggest influence in the particular category. \n",
    "\n",
    "*Or, by a simple rethought, the regions that are highlighted correspond to the class that the CNN describes this image to be.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Libraries\n",
    "\n",
    "Many libraries used in our implementation would be considered as \"standard\" in Computer Vision projects or courses, but we list out everything in the import order to get a better understanding of what each import is used for:\n",
    "\n",
    "- ``PIL``: Used to read images from a directory. This image will get passed into the tensor layers. \n",
    "\n",
    "- ``torch`` / ``torchvision``: The main libraries for PyTorch (along with its own packages). These provide pre-set models (like ResNet18), and ability to create transformations and our own CNNs. This is extensively used for manipulating our tensors (along with ``numpy``, which is more forward-facing as will be seen), providing loaders for our training and testing process, and to perform training & computations on CUDA/MPS (GPU configurations) or the CPU.\n",
    "\n",
    "- ``numpy``: Used to manipulate tensors from ``torch``, and acts as a middle layer to write data in a form that libarries such as ``cv2`` and ``PIL`` can understand. \n",
    "\n",
    "- ``cv2``: Used for dealing with image resizing, writing/drawing, and modifying. It is particularly useful in overlaying our heatmap on top of the image, and optionally writing an image to a directory for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use a dataset of face and non-face images found on Kaggle from Sagar Karar. To get this dataset and format it in the way that the program needs to read it, run the following commands.\n",
    "\n",
    "**Note**: The first step assumes that you have the ``kaggle`` package installed on pip. Otherwise, click on [this link to the dataset page](https://www.kaggle.com/datasets/sagarkarar/nonface-and-face-dataset) and download the dataset. This will replace the first line in the bash script below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kaggle datasets download -d sagarkarar/nonface-and-face-dataset\n",
    "unzip nonface-and-face-dataset.zip\n",
    "cd Dataset\n",
    "\n",
    "# Make directories to split face and no face labels (trained with human, cat, dog faces)\n",
    "mkdir face\n",
    "mkdir no_face\n",
    "mv Human* Cat* Dog* face/\n",
    "mv *.* no_face/\n",
    "\n",
    "\n",
    "# Creating directories\n",
    "mkdir -p train/face\n",
    "mkdir -p train/no_face\n",
    "mkdir -p test/face\n",
    "mkdir -p test/no_face\n",
    "\n",
    "\n",
    "# Move 80% of face images to train/face and 20% to test/face\n",
    "find face -type f | sort -R | head -n $(($(find face -type f | wc -l) * 80 / 100)) | xargs -I {} mv {} train/face/\n",
    "mv face/* test/face/\n",
    "\n",
    "# Move 80% of no face images to train/no_face and 20% to test/no_face\n",
    "find no_face -type f | sort -R | head -n $(($(find no_face -type f | wc -l) * 80 / 100)) | xargs -I {} mv {} train/no_face/\n",
    "mv no_face/* test/no_face/\n",
    "\n",
    "rm -r face no_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre-trained model used: ResNet 18\n",
    "# model = models.resnet(pretrained=True)\n",
    "# final_convolution_layer = 'layer4'\n",
    "# model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # img = cv2.imread('river_hand.jpeg')\n",
    "\n",
    "# if img is not None:\n",
    "#     print(\"Image loaded successfully!\")\n",
    "# else:\n",
    "#     print(\"Unable to load the image. Please check the file path.\")\n",
    "    \n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a uniform size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load train and test datasets\n",
    "train_dataset = ImageFolder('Dataset/train', transform=transform)\n",
    "test_dataset = ImageFolder('Dataset/test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fanrongqi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/fanrongqi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 output classes: face and no-face\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fanrongqi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1183\n",
      "Epoch [2/5], Loss: 0.0264\n",
      "Epoch [3/5], Loss: 0.0315\n",
      "Epoch [4/5], Loss: 0.0444\n",
      "Epoch [5/5], Loss: 0.0151\n",
      "Test Accuracy: 99.06%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# Remove the fully connected layer\n",
    "model2 = nn.Sequential(*list(model.children())[:-2])\n",
    "# model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.87%\n",
      "output tensor([[ 8.9453, -9.5607]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "1.000 -> face\n",
      "0.000 -> no_face\n",
      "output CAM.jpg for the top1 prediction: face\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "finalconv_name = 'layer4'\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "LABELS_file = 'imagenet-simple-labels.json'\n",
    "image_file = 'sat.png'\n",
    "\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(model.parameters())\n",
    "# print(params)\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file)\n",
    "if img_pil.mode == \"RGBA\":\n",
    "    img_pil = img_pil.convert(\"RGB\")\n",
    "img_tensor = preprocess(img_pil)\n",
    "# print(\"img\", img_tensor)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0)).to(device)\n",
    "logit = model(img_variable)\n",
    "\n",
    "# load the imagenet category list\n",
    "# with open(LABELS_file) as f:\n",
    "#     classes = json.load(f)\n",
    "\n",
    "print(\"output\", logit)\n",
    "\n",
    "classes = ['face', 'no_face']\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.cpu().numpy()\n",
    "idx = idx.cpu().numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 2):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread('sat.png')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM2.jpg', result)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(result)\n",
    "\n",
    "# print(heatmap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully!\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (7,7) and (512,49) not aligned: 7 (dim 1) != 512 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     cam \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(weight \u001b[38;5;241m*\u001b[39m feature_maps\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# cam = np.maximum(cam, 0)  # ReLU activation\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_maps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m cam \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(cam, (img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     69\u001b[0m cam \u001b[38;5;241m=\u001b[39m cam \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(cam)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (7,7) and (512,49) not aligned: 7 (dim 1) != 512 (dim 0)"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "# # Remove the fully connected layer\n",
    "# model = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread('sat.png')\n",
    "# img = cv2.imread('river_hand.jpeg')\n",
    "# img = cv2.imread('image_2.jpg')\n",
    "# img = cv2.imread('tejas.jpg')\n",
    "# img = cv2.imread('shahan.jpg')\n",
    "# img = cv2.imread('osama.jpg')\n",
    "# img = cv2.imread('Human1250 copy.png')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_img = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "# print(\"input image\", input_img)\n",
    "\n",
    "# Forward pass to get feature maps\n",
    "with torch.no_grad():\n",
    "    feature_maps = model2(input_img)\n",
    "\n",
    "# print(\"feature map\", feature_maps)\n",
    "# Get the weights of the final convolutional layer\n",
    "final_conv_layer = None\n",
    "for layer in reversed(model2):\n",
    "    # if isinstance(layer, torch.nn.modules.container.Sequential):\n",
    "    #     for l in reversed(layer):\n",
    "    #         print(\"inside sequential\", l)\n",
    "    #         for d in reversed(l):\n",
    "    #             print(\"inside sequential in\", d)\n",
    "    #             # if isinstance(d, torch.nn.modules.conv.Conv2d):\n",
    "    #             #     final_conv_layer = d\n",
    "\n",
    "    #     break\n",
    "    if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "        final_conv_layer = layer\n",
    "        break\n",
    "\n",
    "print(final_conv_layer)\n",
    "if final_conv_layer is None:\n",
    "    raise ValueError(\"Final convolutional layer not found in the model.\")\n",
    "\n",
    "final_conv_layer_weights = final_conv_layer.weight.detach().cpu()\n",
    "\n",
    "# Compute the class activation map (CAM)\n",
    "cam = np.zeros((feature_maps.shape[2], feature_maps.shape[3]), dtype=np.float32)\n",
    "for i in range(final_conv_layer_weights.size(0)):\n",
    "    weight = final_conv_layer_weights[i].detach().cpu().numpy()\n",
    "    cam += np.sum(weight * feature_maps.squeeze(0)[i].cpu().numpy(), axis=0)\n",
    "\n",
    "# cam = np.maximum(cam, 0)  # ReLU activation\n",
    "cam = weight[0].dot(feature_maps[0].cpu().numpy().reshape(-1, 7 * 7))\n",
    "cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "\n",
    "# Apply heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "\n",
    "print(heatmap.shape)\n",
    "# heatmap = np.flip(heatmap, axis=0)\n",
    "superimposed_img = heatmap * 0.3 + img.astype('float32') * 0.5\n",
    "superimposed_img = superimposed_img / superimposed_img.max()\n",
    "\n",
    "# Display the original image and the image with the heatmap\n",
    "# cv2.imshow('Original Image', img)\n",
    "# cv2.imshow('CAM', np.uint8(255 * superimposed_img))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(img)\n",
    "\n",
    "plt.imshow(np.uint8(255 * superimposed_img))\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully!\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "feature blobs 4\n",
      "torch.Size([1, 2])\n",
      "weight shape (2, 512)\n",
      "cam [-1.5298108  -2.39165    -3.2859166  -4.1571302  -4.6499615  -4.323335\n",
      " -3.044087   -1.3582792  -2.4410014  -3.5968165  -4.95731    -6.013628\n",
      " -5.8265295  -4.0293384   0.43399006 -0.24567194 -1.8407055  -3.644533\n",
      " -5.166776   -5.337719   -3.8092735   5.980333    6.6288147   2.680448\n",
      " -2.1708903  -3.6817536  -4.2229466  -3.2590427   9.587239   11.355502\n",
      "  5.983288   -0.03198629 -2.131022   -2.8677545  -2.4269586   9.3137665\n",
      " 10.852405    6.886322    3.3129153   1.4106956  -0.9599694  -1.477473\n",
      "  5.167238    6.006291    4.8635964   4.2122927   3.4761846   0.8496034\n",
      " -0.3901264 ]\n",
      "shape (3024, 4032)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "# # Remove the fully connected layer\n",
    "# model = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# Load and preprocess the image\n",
    "# img = cv2.imread('sat.png')\n",
    "img = cv2.imread('2 faces.png')\n",
    "# img = cv2.imread('river_hand.jpeg')\n",
    "# img = cv2.imread('image_2.jpg')\n",
    "# img = cv2.imread('tejas.jpg')\n",
    "# img = cv2.imread('shahan.jpg')\n",
    "# img = cv2.imread('osama.jpg')\n",
    "# img = cv2.imread('Human1250 copy.png')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "print(model.eval())\n",
    "\n",
    "model._modules.get('layer4').register_forward_hook(hook_feature)\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_img = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# Forward pass to get feature maps\n",
    "with torch.no_grad():\n",
    "    feature_maps = model(input_img)\n",
    "\n",
    "# # Get the weights of the final convolutional layer\n",
    "# final_conv_layer = None\n",
    "# for layer in reversed(model2):\n",
    "#     # print(\"layer\", layer)\n",
    "#     if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "#         final_conv_layer = layer\n",
    "#         break\n",
    "\n",
    "print(\"feature blobs\", len(features_blobs))\n",
    "print(feature_maps.shape)\n",
    "\n",
    "# print(final_conv_layer)\n",
    "# if final_conv_layer is None:\n",
    "#     raise ValueError(\"Final convolutional layer not found in the model.\")\n",
    "\n",
    "# final_conv_layer_weights = final_conv_layer.weight.detach().cpu()\n",
    "\n",
    "# Compute the class activation map (CAM)\n",
    "# cam = np.zeros((feature_maps.shape[2], feature_maps.shape[3]), dtype=np.float32)\n",
    "# for i in range(final_conv_layer_weights.size(0)):\n",
    "#     weight = final_conv_layer_weights[i].detach().cpu().numpy()\n",
    "#     # print(weight.shape)\n",
    "#     params = list(model.parameters())\n",
    "#     # print(params)\n",
    "#     weight = np.squeeze(params[-2].data.cpu().numpy())\n",
    "#     print(feature_maps.squeeze(0)[i].cpu().numpy().shape)\n",
    "#     cam += np.sum(weight * feature_maps.squeeze(0)[i].cpu().numpy(), axis=0)\n",
    "\n",
    "\n",
    "params = list(model.parameters())\n",
    "#     # print(params)\n",
    "weight = np.squeeze(params[-2].data.cpu().numpy())\n",
    "print(\"weight shape\", weight.shape)\n",
    "# cam = np.sum(weight[0].T * feature_maps[0].cpu().numpy(), axis=0)\n",
    "# cam = weight[0].dot(feature_maps[0].cpu().numpy().reshape(-1, 7 * 7))\n",
    "cam = weight[0].dot(features_blobs[0].reshape(-1, 7 * 7))\n",
    "\n",
    "print(\"cam\", cam)\n",
    "# cam = np.maximum(cam, 0)  # ReLU activation\n",
    "# cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "cam = cam.reshape(7, 7)\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "cam = np.uint8(255 * cam)\n",
    "cam = cv2.resize(cam, (256, 256))\n",
    "cam = cv2.resize(cam, (img.shape[1], img.shape[0])) \n",
    "print(\"shape\", cam.shape)\n",
    "\n",
    "# Apply heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM3.jpg', result)\n",
    "\n",
    "# print(heatmap.shape)\n",
    "# # heatmap = np.flip(heatmap, axis=0)\n",
    "# superimposed_img = heatmap * 0.3 + img.astype('float32') * 0.5\n",
    "# superimposed_img = superimposed_img / superimposed_img.max()\n",
    "\n",
    "# # Display the original image and the image with the heatmap\n",
    "# # cv2.imshow('Original Image', img)\n",
    "# # cv2.imshow('CAM', np.uint8(255 * superimposed_img))\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.imshow(img)\n",
    "\n",
    "# plt.imshow(np.uint8(255 * superimposed_img))\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight[0].dot(feature_maps[0].cpu().numpy().reshape(-1, 7 * 7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fanrongqi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/fanrongqi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully!\n",
      "(395, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Remove the fully connected layer\n",
    "model = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread('image_1.jpg')\n",
    "\n",
    "if img is not None:\n",
    "    print(\"Image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Unable to load the image. Please check the file path.\")\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_img = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Forward pass to get feature maps\n",
    "with torch.no_grad():\n",
    "    feature_maps = model(input_img)\n",
    "\n",
    "# Get the weights of the final convolutional layer\n",
    "final_conv_layer = None\n",
    "for layer in reversed(model):\n",
    "    if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "        final_conv_layer = layer\n",
    "        break\n",
    "\n",
    "if final_conv_layer is None:\n",
    "    raise ValueError(\"Final convolutional layer not found in the model.\")\n",
    "\n",
    "final_conv_layer_weights = final_conv_layer.weight.detach().cpu()\n",
    "\n",
    "# Compute the class activation map (CAM)\n",
    "cam = np.zeros((feature_maps.shape[2], feature_maps.shape[3]), dtype=np.float32)\n",
    "for i in range(final_conv_layer_weights.size(0)):\n",
    "    weight = final_conv_layer_weights[i].detach().cpu().numpy()\n",
    "    cam += np.sum(weight * feature_maps.squeeze(0)[i].cpu().numpy(), axis=0)\n",
    "\n",
    "cam = np.maximum(cam, 0)  # ReLU activation\n",
    "cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "\n",
    "# Apply heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "print(heatmap.shape)\n",
    "superimposed_img = heatmap * 0.4 + img.astype('float32') * 0.6\n",
    "superimposed_img = superimposed_img / superimposed_img.max()\n",
    "\n",
    "# Display the original image and the image with the heatmap\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('CAM', np.uint8(255 * superimposed_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
